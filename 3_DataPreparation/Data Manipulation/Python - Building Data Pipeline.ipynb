{"cells":[{"source":"![walmartecomm](walmartecomm.jpg)\n\nWalmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n\nIn this project, you have been tasked with creating a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. You will be working with two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table in `PostgreSQL` database with the following features:\n\n# `grocery_sales`\n- `\"index\"` - unique ID of the row\n- `\"Store_ID\"` - the store number\n- `\"Date\"` - the week of sales\n- `\"Weekly_Sales\"` - sales for the given store\n\nAlso, you have the `extra_data.parquet` file that contains complementary data:\n\n# `extra_data.parquet`\n- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n- `\"Temperature\"` - Temperature on the day of sale\n- `\"Fuel_Price\"` - Cost of fuel in the region\n- `\"CPI\"` â€“ Prevailing consumer price index\n- `\"Unemployment\"` - The prevailing unemployment rate\n- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n- `\"Dept\"` - Department Number in each store\n- `\"Size\"` - size of the store\n- `\"Type\"` - type of the store (depends on `Size` column)\n\nYou will need to merge those files and perform some data manipulations. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n- `\"Store_ID\"`\n- `\"Month\"`\n- `\"Dept\"`\n- `\"IsHoliday\"`\n- `\"Weekly_Sales\"`\n- `\"CPI\"`\n- \"`\"Unemployment\"`\"\n\nAfter merging and cleaning the data, you will have to analyze monthly sales of Walmart and store the results of your analysis as the `agg_data` variable that should look like:\n\n|  Month | Weekly_Sales  | \n|---|---|\n| 1.0  |  33174.178494 |\n|  2.0 |  34333.326579 |\n|  ... | ...  |  \n\nFinally, you should save the `clean_data` and `agg_data` as the csv files.\n\nIt is recommended to use `pandas` for this project. ","metadata":{},"id":"ef36f535-4bdc-4e2b-a22a-179372324b26","cell_type":"markdown"},{"source":"-- Write your SQL query here\nSELECT * FROM grocery_sales\nLIMIT 10","metadata":{"customType":"sql","dataFrameVariableName":"grocery_sales","sqlCellMode":"dataFrame","sqlSource":{"integrationId":"89e17161-a224-4a8a-846b-0adc0fe7a4b1","type":"integration"},"executionCancelledAt":null,"executionTime":928,"lastExecutedAt":1738152754746,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"-- Write your SQL query here\nSELECT * FROM grocery_sales\nLIMIT 10","outputsMetadata":{"0":{"height":550,"type":"dataFrame","tableState":{"quickFilterText":""}}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b"},"id":"59fe49dc-cda5-4d22-bb10-49e94cdb6437","cell_type":"code","execution_count":35,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"level_0","type":"integer"},{"name":"index","type":"integer"},{"name":"Store_ID","type":"integer"},{"name":"Date","type":"datetime"},{"name":"Dept","type":"integer"},{"name":"Weekly_Sales","type":"number"}],"primaryKey":["level_0"],"pandas_version":"1.4.0"},"data":{"level_0":[0,1,2,3,4,5,6,7,8,9],"index":[0,1,2,3,4,5,6,7,8,9],"Store_ID":[1,1,1,1,1,1,1,1,1,1],"Date":["2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000","2010-02-05T00:00:00.000"],"Dept":[1,26,17,45,28,79,55,5,58,7],"Weekly_Sales":[24924.5,11737.12,13223.76,37.44,1085.29,46729.77,21249.31,32229.38,7659.97,null]}},"total_rows":10,"truncation_type":null},"text/plain":"   index  Store_ID       Date  Dept  Weekly_Sales\n0      0         1 2010-02-05     1      24924.50\n1      1         1 2010-02-05    26      11737.12\n2      2         1 2010-02-05    17      13223.76\n3      3         1 2010-02-05    45         37.44\n4      4         1 2010-02-05    28       1085.29\n5      5         1 2010-02-05    79      46729.77\n6      6         1 2010-02-05    55      21249.31\n7      7         1 2010-02-05     5      32229.38\n8      8         1 2010-02-05    58       7659.97\n9      9         1 2010-02-05     7           NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Store_ID</th>\n      <th>Date</th>\n      <th>Dept</th>\n      <th>Weekly_Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>1</td>\n      <td>24924.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>26</td>\n      <td>11737.12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>17</td>\n      <td>13223.76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>45</td>\n      <td>37.44</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>28</td>\n      <td>1085.29</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>79</td>\n      <td>46729.77</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>55</td>\n      <td>21249.31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>5</td>\n      <td>32229.38</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>58</td>\n      <td>7659.97</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>1</td>\n      <td>2010-02-05</td>\n      <td>7</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":35,"@datacamp/metadata":{"executedQuery":"-- Write your SQL query here\nSELECT * FROM grocery_sales\nLIMIT 10","executedQueryParameters":[]}}]},{"source":"import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")","metadata":{"executionCancelledAt":null,"executionTime":73,"lastExecutedAt":1738152754819,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport os\n\n# Extract function is already implemented for you \ndef extract(store_data, extra_data):\n    extra_df = pd.read_parquet(extra_data)\n    merged_df = store_data.merge(extra_df, on = \"index\")\n    return merged_df\n\n# Call the extract() function and store it as the \"merged_df\" variable\nmerged_df = extract(grocery_sales, \"extra_data.parquet\")"},"id":"c0d64ff1-a4ca-4a82-a8b4-e210244dedc1","cell_type":"code","execution_count":36,"outputs":[]},{"source":"# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n  # Fill NaNs using mean since we are dealing with numeric columns\n  # Set inplace = True to do the replacing on the current DataFrame\n    raw_data.fillna(\n      {\n          'CPI': raw_data['CPI'].mean(),\n          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n          'Unemployment': raw_data['Unemployment'].mean(),\n      }, inplace = True\n    )\n\n    # Define the type of the \"Date\" column and its format\n    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n\n    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n    \n    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n    return raw_data","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1738152754875,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the transform() function with one parameter: \"raw_data\"\ndef transform(raw_data):\n  # Fill NaNs using mean since we are dealing with numeric columns\n  # Set inplace = True to do the replacing on the current DataFrame\n    raw_data.fillna(\n      {\n          'CPI': raw_data['CPI'].mean(),\n          'Weekly_Sales': raw_data['Weekly_Sales'].mean(),\n          'Unemployment': raw_data['Unemployment'].mean(),\n      }, inplace = True\n    )\n\n    # Define the type of the \"Date\" column and its format\n    raw_data[\"Date\"] = pd.to_datetime(raw_data[\"Date\"], format = \"%Y-%m-%d\")\n    # Extract the month value from the \"Date\" column to calculate monthly sales later on\n    raw_data[\"Month\"] = raw_data[\"Date\"].dt.month\n\n    # Filter the entire DataFrame using the \"Weekly_Sales\" column. Use .loc to access a group of rows\n    raw_data = raw_data.loc[raw_data[\"Weekly_Sales\"] > 10000, :]\n    \n    # Drop unnecessary columns. Set axis = 1 to specify that the columns should be removed\n    raw_data = raw_data.drop([\"index\", \"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"Type\", \"Size\", \"Date\"], axis = 1)\n    return raw_data"},"id":"6d3c25e2-e7d8-4c33-9be0-d45f03b2cf43","cell_type":"code","execution_count":37,"outputs":[]},{"source":"# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1738152754927,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the transform() function and pass the merged DataFrame\nclean_data = transform(merged_df)"},"id":"620b7289-06cd-4205-be9e-a50dc8d36cf0","cell_type":"code","execution_count":38,"outputs":[]},{"source":"# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n  \t# Select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis\n    holidays_sales = clean_data[[\"Month\", \"Weekly_Sales\"]]\n   \t# Create a chain operation with groupby(), agg(), reset_index(), and round() functions\n    # Group by the \"Month\" column and calculate the average monthly sales\n    # Call reset_index() to start a new index order\n    # Round the results to two decimal places\n    \n    holidays_sales = (holidays_sales.groupby(\"Month\")\n    .agg(Avg_Sales = (\"Weekly_Sales\", \"mean\"))\n    .reset_index().round(2))\n    return holidays_sales","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1738152754979,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the avg_weekly_sales_per_month function that takes in the cleaned data from the last step\ndef avg_weekly_sales_per_month(clean_data):\n  \t# Select the \"Month\" and \"Weekly_Sales\" columns as they are the only ones needed for this analysis\n    holidays_sales = clean_data[[\"Month\", \"Weekly_Sales\"]]\n   \t# Create a chain operation with groupby(), agg(), reset_index(), and round() functions\n    # Group by the \"Month\" column and calculate the average monthly sales\n    # Call reset_index() to start a new index order\n    # Round the results to two decimal places\n    \n    holidays_sales = (holidays_sales.groupby(\"Month\")\n    .agg(Avg_Sales = (\"Weekly_Sales\", \"mean\"))\n    .reset_index().round(2))\n    return holidays_sales"},"id":"b19b15e3-6624-47a9-927f-d3f12fe8212d","cell_type":"code","execution_count":39,"outputs":[]},{"source":"# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1738152755030,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the avg_weekly_sales_per_month() function and pass the cleaned DataFrame\nagg_data = avg_weekly_sales_per_month(clean_data)"},"id":"fe875e27-b0cf-4e52-994e-4ae1fe6e8876","cell_type":"code","execution_count":40,"outputs":[]},{"source":"# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n    full_data.to_csv(full_data_file_path, index = False)\n    agg_data.to_csv(agg_data_file_path, index = False)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1738152755082,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the load() function that takes in the cleaned DataFrame and the aggregated one with the paths where they are going to be stored\ndef load(full_data, full_data_file_path, agg_data, agg_data_file_path):\n  \t# Save both DataFrames as csv files. Set index = False to drop the index columns\n    full_data.to_csv(full_data_file_path, index = False)\n    agg_data.to_csv(agg_data_file_path, index = False)"},"id":"921cb123-3153-4334-bdeb-9bb227fdc530","cell_type":"code","execution_count":41,"outputs":[]},{"source":"# Call the load() function and pass the cleaned and aggregated DataFrames with their paths\nload(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1738152755131,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the load() function and pass the cleaned and aggregated DataFrames with their paths\nload(clean_data, \"clean_data.csv\", agg_data, \"agg_data.csv\")"},"id":"f518ad5c-214e-474b-80bd-827b0c0e1536","cell_type":"code","execution_count":42,"outputs":[]},{"source":"# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n  \t# Use the \"os\" package to check whether a path exists\n    file_exists = os.path.exists(file_path)\n    # Raise an exception if the path doesn't exist, hence, if there is no file found on a given path\n    if not file_exists:\n        raise Exception(f\"There is no file at the path {file_path}\")","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1738152755178,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create the validation() function with one parameter: file_path - to check whether the previous function was correctly executed\ndef validation(file_path):\n  \t# Use the \"os\" package to check whether a path exists\n    file_exists = os.path.exists(file_path)\n    # Raise an exception if the path doesn't exist, hence, if there is no file found on a given path\n    if not file_exists:\n        raise Exception(f\"There is no file at the path {file_path}\")"},"id":"61b5f58a-70cb-40b3-bdbe-20b4079276e3","cell_type":"code","execution_count":43,"outputs":[]},{"source":"# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation(\"clean_data.csv\")\nvalidation(\"agg_data.csv\")","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1738152755227,"lastExecutedByKernel":"8420693b-c6f8-4f60-8b02-cdb008e1b20b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Call the validation() function and pass first, the cleaned DataFrame path, and then the aggregated DataFrame path\nvalidation(\"clean_data.csv\")\nvalidation(\"agg_data.csv\")"},"id":"df1659ff-41c4-4a92-9812-80c6eaa02b90","cell_type":"code","execution_count":44,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}